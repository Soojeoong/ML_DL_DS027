# 4.2 k-최근접 이웃(k-Nearest Neighbor, kNN)
- **데이터 분류에 사용되는 아주 간단한 지도학습 알고리즘**
	- 지도학습 : 머신러닝 학습 시 데이터와 함께 데이터에 대한 레이블(정답을 함께 부여하는 학습 방식)
	- 데이터 분류 : 새로운 데이터를 기존 데이터의 레이블 중 하나로 분류하는 작
- 예 : 손글씨 숫자 인식
	- 숫자 인식 모델을 학습실킬 때 학습할 데이터와 함께 그 데이터가 무슨 숫자를 의미하는지 명시적인 레이블과 함께 학습 시킨다.(지도학습)
	- 학습이 완료된 모델은 입력된 손글씨 숫자에 대해 분류를 진행하는데, 이 분류값은 학습에 사용됐던 기존 데이터들의 레이블 중 하나에 해당한다.(데이터 분류)
- *장점 : 상대적으로 이해하기 쉽다.*
- *단점 : 연산 속도가 느리다.*

## ✅ kNN 알고리즘의 이론
- **kNN 알고리즘 : 현재 데이터를 특정값으로 분류하기 위해 기존의 데이터 안에서 현재 데이터로부터 가까운 k개의 데이터를 찾아 k개의 레이블 중 가장 많이 분류된 값으로 현재의 데이터를 분류하는 알고리즘**

- 현실 공간 vs 벡터 공간
	- 현실 공간 : 평면 이동 및 수직 이동이 가능한 3차원 공간
	- 벡터 공간 : 벡터 연산이 가능한 N차원 공간
- 머신러닝에서의 공간 -> 벡터 공간

- **k(탐색할 이웃의 개수)**에 따라 데이터를 다르게 예측할 수도 있다.
- 과반수 이상의 이웃이 나오게 하기 위해 k를 보통 **홀수**도 설정한다.
- 최적의 k를 찾기 위해 보통 검증 데이터를 통해 **가장 정확도가 높은 k**를 kNN 알고리즘의 k로 선정한다.

- **이진 분류 vs 다중 분류**

|이진분류 | 다중분류|
| --- | ---|
| 악성 코드 분류(일반 파일 vs 악성 코드) | 손글씨 숫자 분류(1~9)|
| 위조 지폐 분류(일반 지폐 vs 위조 지폐) | 서울 도시 분류 (강동, 강서, 강남, 강북) |
| 문장에서 사람 감정 분류(행복 vs 슬픔) | 감정 분류(행복, 슬픔, 화남)

## ✅ kNN 알고리즘의 수학적 이해
-  kNN 알고리즘 : 거리 기반 알고리즘
- 거리 측정 방법
	1. **유클리드 거리(Euclidean Distance)**
	$$d(A,B) = (x2−x1)^2 + (y2−y1)^2$$

	2. **맨해튼 거리(Manhattan Distance)**
	$$d(A,B)=∣x1−x2∣+∣y1−y2∣$$

## ✅ kNN 알고리즘의 장단점
- 👉 장점
	1. **이해하기가 상당히 쉽다.**
		- 다른 알고리즘은 미적분, 확률 및 정보 이론 등의 기본 지식이 필요하지만 kNN은 수학적 거리 계산법만 알면 이해하기 쉽다.
	2. **숫자로 구분된 속성에 우수한 성능을 보인다.**
		-거리, 획수, 점수 등 수치화된 데이터는 정확도가 높다.
	3. **별도의 모델 학습이 필요 없다.**
		- 예측을 하는 시점에서 모든 기존 데이터와의 거리를 계산하므로
		- 게으른 학습(lazy learning)이라고도 한다. 게으른 학습은 데이터베이스의 실시간 데이터를 사욯해야 할 때 유용하게 쓰인다.
- 👉 단점
	1. **예측 속도가 느리다.**
		- 전체 데이터와의 거리를 계산하기 때문에
		- 비교할 속성이 많아질수록 연산 속도는 더 느려진다.
	2. **측값이 지역 정보에 많이 편향될 수 있다.**
		- 다른 알고리즘은 예측값이 기존의 전체 데이터에서 학습된 모델에서 나오지만 kNN은 가까운 이웃을 통해 예측하므로
		- k의 개수가 적거나 몇 개의 예외적인 데이터가 이웃으로 존재할 경우 예측값이 틀릴 가능성이 높아진다.

## ✅ 실습
- [🏀농구선수의 게임 데이터를 활용한 포지션 예측](./실습/kNN실습.ipynb)
