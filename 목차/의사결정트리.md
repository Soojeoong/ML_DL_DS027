# 4.4 의사결정 트리
- 데이터 분류 및 회귀에 사용되는 지도학습 알고리즘
- **장점** : 왜, 어떻게 나왔는지 이해하기 쉽다, 정확도가 높다.
- **단점** : 과대적합되기 쉽다.

## ✅ 의사결정 트리 이론
- 머신러닝에서 의미 있는 질문 : 데이터의 특징 중 의미 있는 특징
- 영향력이 큰 특징을 상위 노드로, 영향력이 작은 특징을 하위 노드로 선택한다.
- 이렇게 트리가 완성된 후, 단순히 입력된 데이터의 특징에 맞게 트리를 순회해서 해당 레이블을 찾는다.

>### ⭐ 영향력 있는 특징 고르는 방법
1️⃣ 정보 엔트로피
2️⃣ 지니 계수

### 1️⃣  정보 엔트로피
- 약간씩의 정보를 획득하는 과정은 정답에 대한 불확실성이 조금씩 줄어드는 것이다.
- 엔트로피(entroypy) : 정보 이론(information Theory)에서 이 **불확실성을 수치적으로** 표현한 값
- 정보 이득(information gain) : 질문 이전의 엔트로피에서 질문 후의 엔트로피를 뺀 값, 즉 불확실성이 줄어 든 정도
```
질문 후의 정보 이득 = 질문 전의 엔트로피 - 질문 후의 엔트로피
Gain(T,X) = Entropy(T) - Entropy(T,X)
```
#### 확률을 바탕으로 정보 엔트로피를 구하는 공식
$$ Entropy = \sum_{i=1}^{m} - \rho_{i}log(2)\rho_{i}$$

#### 의사결정 트리 사례로 정보 엔트로피 구하기
- 의사결정 트리의 최초 엔트로피 : 1 

#### 한 가지 특징에 대한 엔트로피 계산
- 특징을 가지고 분류할 수 있으므로 특징이 한 가지일 때 엔트로피를 계산할 수 있다면 해당 특징을 활용했을 때의 정보 이득도 알아낼 수 있다.

#### 특징에 대한 엔트로피를 계산하는 공식
- 하나의 특징으로 데이터를 분리했을 때 엔트로피 계산
$$Entropy = \sum_{c \in X}P(c)E(c)$$
```
- X: 선택된 특징
- c: 선택된 특징에 의해 생성되는 하위 노드
- P(c): 선택된 특징에 의해 생성된 하위 노드에 데이터가 속할 확률
- E(c): 선택된 특징에 의해 생성된 하위 노드의 엔트로피
```

### 2️⃣ 지니 계수(Gini coefficient)
1. 특징이 항상 이진 분류로 나뉠 때 사용됨
2. 지니 계수가 높을 수록 순도가 높음
- 순도가 높음 : 한 그룹에 모여있는 데이터들의 속성들이 많이 일치한다.
- 불순도가 높음 : 한 그룹에 여러 속성의 데이터가 많이 섞여 있다

#### 사이킷런의 의사결정 트리
- CART(classification and regression tree)타입의 의사결정 트리이며,
- CART는 트리의 노드마다 특징을 이진 분류하는 특징이 있어 사이킷런은 트리를 구성할 때 기본적으로 지니 계수 사용한다!

#### 지니 계수를 통해 의사결정 트리의 노드 결정하는 순서
1. 특징으로 분리된 두 노드의 지니 계수를 구함(P^2 + Q^2)
2. 특징에 대한 지니 계수를 구함



