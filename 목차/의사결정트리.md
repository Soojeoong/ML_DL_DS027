# 4.4 의사결정 트리
- 데이터 분류 및 회귀에 사용되는 지도학습 알고리즘
- **장점** : 왜, 어떻게 나왔는지 이해하기 쉽다, 정확도가 높다.
- **단점** : 과대적합되기 쉽다.

## ✅ 의사결정 트리 이론
- 머신러닝에서 의미 있는 질문 : 데이터의 특징 중 의미 있는 특징
- 영향력이 큰 특징을 상위 노드로, 영향력이 작은 특징을 하위 노드로 선택한다.
- 이렇게 트리가 완성된 후, 단순히 입력된 데이터의 특징에 맞게 트리를 순회해서 해당 레이블을 찾는다.

### ⭐ 영향력 있는 특징 고르는 방법
1. 정보 엔트로피
2. 지니 계수

### 1. 정보 엔트로피
- 약간씩의 정보를 획득하는 과정은 정답에 대한 불확실성이 조금씩 줄어드는 것이다.
- 엔트로피(entroypy) : 정보 이론(information Theory)에서 이 **불확실성을 수치적으로** 표현한 값
- 정보 이득(information gain) : 질문 이전의 엔트로피에서 질문 후의 엔트로피를 뺀 값, 즉 불확실성이 줄어 든 정도
```
질문 후의 정보 이득 = 질문 전의 엔트로피 - 질문 후의 엔트로피
Gain(T,X) = Entropy(T) - Entropy(T,X)
```
#### 1️⃣ 확률을 바탕으로 정보 엔트로피를 구하는 공식
$$ Entropy = \sum_{i=1}^{m} - \rho_{i}log(2)\rho_{i}$$

#### 2️⃣ 의사결정 트리 사례로 정보 엔트로피 구하기
- 의사결정 트리의 최초 엔트로피 : 1 

#### 3️⃣ 한 가지 특징에 대한 엔트로피 계산
- 특징을 가지고 분류할 수 있으므로 특징이 한 가지일 때 엔트로피를 계산할 수 있다면 해당 특징을 활용했을 때의 정보 이득도 알아낼 수 있다.

#### 4️⃣ 특징에 대한 엔트로피를 계산하는 공식
- 하나의 특징으로 데이터를 분리했을 때 엔트로피 계산
$$Entropy = \sum_{c \in X}P(c)E(c)$$
```
- X: 선택된 특징
- c: 선택된 특징에 의해 생성되는 하위 노드
- P(c): 선택된 특징에 의해 생성된 하위 노드에 데이터가 속할 확률
- E(c): 선택된 특징에 의해 생성된 하위 노드의 엔트로피
```

### 2. 지니 계수
