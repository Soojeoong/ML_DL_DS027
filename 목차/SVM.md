# 4.3 서포트 벡터 머신 (SVM)
- **사용하기 편하면서 높은 정확도를 보이는 지도학습 머신러닝 알고리즘**

## ✅ 서포트 벡터 머신의 이론
- 결정 경계선이 데이터 포인트에 가까이 위치할수록 조금의 속성 차이에도 분류값이 달라질 수 있기 때문에 예측의 정확도가 불안정해진다.
- **결정 경계선**
	- 서로 다른 분류값을 결정하는 경계
### 서포트 벡터(support vector)
- 벡터 : 2차원 공간 상에 나타난 데이터 포인트
- 서포트 벡터 : 결정 경계선과 가장 가까이 맞닿은 데이터 포인트

### 마진(margin)
- 서포트 벡터와 결정 경계 사이의 거리
- SVM의 목표 : 마진을 최대로 하는 결정 경계선 찾기(마진이 클수록 현재 알지 못하는 새로운 데이터에 대해 안정적으로 분류할 가능성이 높다)
- 학습 단계 -> 마진을 최대로 하는 결정 경계 찾기
- 테스트 단계 -> 입력되는 새로운 데이터가 결정 경계의 위쪽인지 아래쪽인지에 따라 분류값이 달라진다. 

### 비용(C) 변수
- 약간의 오류를 허용하기 위해 비용 변수를 사용한다.
- 비용(cost)이 낮을수록, 마진을 최대한 높이고, 학습 에러율을 증가시키는 방향으로 결정 경계선을 만든다. (테스트 에러가 적게)
- 비용이 너무 낮으면 -> 과소적합
- 비용이 너무 높으면 -> 과대적합

### 결정 경계
- 결정 경계 = N-1 차원 (데이터가 3차원 공간에 존재하면 결정 경계는 2차원)
- 이러한 이유로 초평면(hyperplane)이라고 칭하기도 한다.

### 커널 트릭
- 매핑 함수 : 저차원의 데이터를 고차원의 데이터로 옮겨주는 함수(실제로는 계산량이 많아 현실적으로 어려움)
- 커널 트릭 : 실제로는 고차원으로 보내지 않지만 보낸 것과 동일한 효과를 줘서 매우 빠른 속도로 결정 경계선을 찾는 방법

### 선형 SVM vs 커널 트릭
- 선형 SVM : 커널을 사용하지 않고 데이터를 분류함. 비용(C)을 조절해서 마진의 크기를 조절할 수 있다.
- 커널 트릭 : 선형 분리가 주어진 차원에서 불가능할 경우 고차원으로 데이터를 옮기는 효과를 통해 결정 경계를 찾는다. 비용(C)과 gamma를 조절해서 마진을 조절할 수 있다.

### 가우시안 RBF 커널 (가장 많이 쓰임)
- 데이터포인트에 적용되는 가우시안 함수의 표준편차를 조정함으로써 결정 경계의 곡률을 조정한다. 
- 이 표준편차 조정 변수 : 감마(gamma)
- 감마가 커지면 데이터포인트별로 허용하는 표준편차가 작아져서 결정 경계가 작아지면서 구부러진다.

### 파라미터 튜닝
- 비용 : 마진 너비 조절 변수. 클수록 마진 너비가 좁아진다.
- 감마 : 커널의 표준 편차 조절 변수. 작을수록 데이터포인트의 영향이 커져서 경계가 완만해지고, 클수록 경계가 구부러진다.

## ✅ 서포트 벡터 머신의 장단점
- 👉 장점
	1. 커널 트릭을 사용함으로써 특성이 다양한 데이터를 분류하는 데 강하다.
	2. 파라미터(C, gamma)를 조정해서 과대적합, 과소적합에 대처할 수 있다.
	3. 적은 한습 데이터로도 딥러닝만큼 정확도가 높은 분류를 기대할 수 있다.
- 👉 단점
	1. 데이터 전처리 과정(data preprocessing)이 상당히 중요하다. (특성이 다른 경우에 전처리 과정을 통해 특성 그대로 벡터 공간에 표현해야 한다.)
	2. 특성이 많을 경우 결정 경계 및 데이터의 시각화가 어렵다.

	
## ✅ 실습
- [🏀농구선수의 게임 기록을 학습해 포지션 예측하기](./SVM실습.ipynb)